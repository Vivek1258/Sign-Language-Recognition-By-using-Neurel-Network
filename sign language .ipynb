{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend\n",
    "#from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D ,Convolution2D \n",
    "from tensorflow.keras.layers import Activation \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'C://Users//hp//Desktop//project//train'\n",
    "validation_data_path = 'C://Users//hp//Desktop//project//test'\n",
    "\n",
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "img_width, img_height = 150, 150\n",
    "batch_size = 32\n",
    "#samples_per_epoch = 1000\n",
    "#validation_steps = 300\n",
    "nb_filters1 = 32 ## Number of convolution filters to use. , Integer, the dimensionality of the output space \n",
    "nb_filters2 = 64\n",
    "conv1_size = 3 \n",
    "conv2_size = 2   \n",
    "pool_size = 2\n",
    "classes_num = 24 # since we have 24 types of sign language symbols in our dataset \n",
    "lr = 0.0004 # leadning rate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(nb_filters1, conv1_size, conv1_size, padding =\"same\", input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "model.add(Convolution2D(nb_filters2, conv2_size, conv2_size, padding =\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(pool_size, pool_size),strides=None, padding='valid', data_format=None))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))  # regularization technique to prevent from overfitting ,temporally dtop out any random neuron fron NN \n",
    "model.add(Dense(classes_num, activation='softmax')) # since it is multiclass classification  (For binary you can use Sigmoid Function and change loss from categorical_crossentropy to binary_crossentropy )\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              \n",
    "              optimizer=optimizers.RMSprop(lr=lr),# for AOA : divide the gradient by a running avarage of its recent magnituide\n",
    "              metrics=['accuracy'])"
   ]
  },
   
   
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_path,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=800,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C://Users//hp//Desktop//project//vivNew_Model.h5')\n",
    "model.save_weights('C://Users//hp//Desktop//project//vivNew_Model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential, load_model\n",
    " \n",
    "alphabets = ['A' ,'B','C','D','E','F','G','H','I','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y']\n",
    "\n",
    "#Define Path\n",
    "model_path = 'C://Users//hp//Desktop//project//vivNew_Model.h5'\n",
    "model_weights_path ='C://Users//hp//Desktop//project//vivNew_Model_weights.h5'\n",
    "img_path = \"C://Users//hp//Desktop//project//006.jpg\"\n",
    "\n",
    "#Load the pre-trained models\n",
    "model = load_model(model_path)\n",
    "model.load_weights(model_weights_path)\n",
    "\n",
    "#Define image parameters\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "x = load_img(img_path, target_size=(img_width,img_height))\n",
    "x = img_to_array(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "arr =model.predict(x) ;\n",
    "print(alphabets[np.where(arr[0] == 1)[0][0]])\n",
    "#arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
